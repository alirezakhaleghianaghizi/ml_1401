{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alirezakhaleghianaghizi/ml_1401/blob/main/RL_Chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWZ3HloS1Ok-"
      },
      "source": [
        "<img src='http://www-scf.usc.edu/~ghasemig/images/sharif.png' alt=\"SUT logo\" width=300 height=300 align=left class=\"saturate\" >\n",
        "\n",
        "<br>\n",
        "<font>\n",
        "<div dir=ltr align=center>\n",
        "<font color=0F5298 size=7>\n",
        "    Artificial Intelligence <br>\n",
        "<font color=2565AE size=5>\n",
        "    Computer Engineering Department <br>\n",
        "    Spring 2023<br>\n",
        "<font color=3C99D size=5>\n",
        "    Practical Assignment 3 - Reinforcement Learning <br>\n",
        "<font color=696880 size=4>\n",
        "    Mohammad Moshtaghi - Ali Salesi - Hossein Goli\n",
        "\n",
        "____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejfGdour1cNK"
      },
      "source": [
        "# Personal Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IM62bqV51dy2"
      },
      "outputs": [],
      "source": [
        "# Set your student number\n",
        "student_number = '99101462'\n",
        "first_name = 'alireza'\n",
        "last_name = 'khaleghi'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3KLkyuZo0tR"
      },
      "source": [
        "# Rules\n",
        "- Make sure that all of your cells can be run perfectly. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z91za1kfo7uB"
      },
      "source": [
        "# Q2: Sentence Generator (100 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki7JqWK_oSNs"
      },
      "source": [
        "<font size=4>\n",
        "Author: Ali Salesi\n",
        "<br/>\n",
        "<font color=red>\n",
        "Please run all the cells.\n",
        "</font>\n",
        "</font>\n",
        "<br/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2vBT4rxeHnM"
      },
      "source": [
        "In this assignment we implement a text generator using RL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQRBpSNJ2ICr"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDg8VW5k3A4Z"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylFoOb7GIRI3"
      },
      "source": [
        "First, lets download the text corpus crawled from `VOA Persian` from 2003 to 2008."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxG1ZP8Gqk2_",
        "outputId": "c2dcac1f-4491-4c90-b7cc-108291e83077"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-27 21:36:25--  https://storage.googleapis.com/danielk-files/farsi-text/merged_files/voa_persian_2003_2008_cleaned.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.68.128, 74.125.24.128, 142.250.4.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.68.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 69708061 (66M) [text/plain]\n",
            "Saving to: ‘voa_persian.txt’\n",
            "\n",
            "voa_persian.txt     100%[===================>]  66.48M  13.3MB/s    in 5.6s    \n",
            "\n",
            "2023-04-27 21:36:31 (11.8 MB/s) - ‘voa_persian.txt’ saved [69708061/69708061]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O \"voa_persian.txt\" \"https://storage.googleapis.com/danielk-files/farsi-text/merged_files/voa_persian_2003_2008_cleaned.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mzh2UPwisjTG",
        "outputId": "20c42bfb-8a98-4d79-907d-c36e36680bcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "488253\n"
          ]
        }
      ],
      "source": [
        "!wc -l voa_persian.txt | awk '{print $1}'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9MQGhw56n-kC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDKxtmaAqvjB",
        "outputId": "245d821a-9569-4ff4-955e-0b2aae1d2683"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "پيمان صلح بين ژاپن و روسيه\n",
            "بنا به گزارشهای منتشره در توکيو، ژاپن و روسيه در زمينه يک پيمان صلح در چارچوبی گسترده توافق کرده اند که رسماً به مخاصمات جنگ دوم جهانی ميان دو کشور پايان خواهند داد.\n",
            "\n",
            "در يکی از اين گزارشها، که از سوی خبرگزاری کيودُو،انتشار يافته، گفته شده است که دو کشور برای رفع اختلافات ديرين خود بر سر چهار جزيره از جزاير زنجيره ای کوريل، بر اساس سه پيمان گذشته خود عمل خواهند کرد.\n",
            "بموجب يکی از اين پيمانها که در سال ۱۹۵۶ امضاء شده، دو تا از اين جزيره ها پس از امضاء يک پيمان صلح به ژاپن پس داده خواهد شد.\n",
            "اما بموجب پيمانی که در سال ۱۹۹۳ به امضاء رسيده، مسئله حاکميت اين چهار جزيره بايستی پيش از امضاء پيمان صلح فيصله يابد.\n",
            "هيچ يک از دو طرف نحوه استفاده از پيمان های پيشين را اعلام نکرده اند.\n",
            "\n",
            "تشکيلات فلسطينی نخستين بودجه رسمی خود را اعلام کرد\n",
            "تشکيلات فلسطينی پس از دو سال نخستين بودجه رسمی خود را اعلام کرد و قول داد برای از ميان برداشتن فساد و پاسخگوئی بيشتر به مردم تلاش کند.\n"
          ]
        }
      ],
      "source": [
        "!head voa_persian.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj9v5jUm2691"
      },
      "source": [
        "### Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx5YwlZar85k"
      },
      "source": [
        "Then we have to normalize and lemmatize the text so we can have a better generalization of semantics in prompt generation.\n",
        "\n",
        "We'll use `hazm` library for this purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWwihzn7rzAC",
        "outputId": "2a3f7a12-b8b9-482c-d62f-615a79189f01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: hazm in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: libwapiti>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (0.2.1)\n",
            "Requirement already satisfied: nltk==3.3 in /usr/local/lib/python3.10/dist-packages (from hazm) (3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk==3.3->hazm) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install hazm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "c_qV0iYsr-lB"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals\n",
        "from hazm import Normalizer, Lemmatizer, word_tokenize\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "normalizer = Normalizer()\n",
        "lemmatizer = Lemmatizer()\n",
        "\n",
        "\n",
        "def normalize(line: str):\n",
        "    line = re.sub(\n",
        "        r'[.{}[\\]؛:«»؟!٬٫٪×،*)(ـ+<>\\'\",`=+\\-?!@#$%^&*()_\\/\\\\\\\\]', '', line.strip())\n",
        "    line = re.sub(r'\\s+', ' ', line.strip())\n",
        "    line = normalizer.normalize(line)\n",
        "    words = word_tokenize(line)\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    line = ' '.join(words)\n",
        "    return line\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bJwjSKcs1eEf",
        "outputId": "6c1524ac-3571-4cc8-cf4f-7f710f4244aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'من خیلی خوشحال #هست و کتاب زیاد درباره یخچال قطب خواند#خوان'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "normalize('من خیلی خوشحال هستم و کتاب‌های زیادی درباره یخچال‌های قطبی خوانده‌ام.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOD9fyM3sK8K",
        "outputId": "61faa7f7-c564-45de-cf83-7c97768f1057"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 488253/488253 [00:47<00:00, 10254.42it/s]\n"
          ]
        }
      ],
      "source": [
        "voa = open('voa_persian.txt')\n",
        "voa_norm = open('voa_persian_normalized.txt', 'w')\n",
        "for i, line in tqdm(enumerate(voa), total=488253):\n",
        "    voa_norm.write(normalize(line) + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df4v9sYDxDT9",
        "outputId": "647a730f-90df-433d-d4e5-6fcacc600ccd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "پیمان صلح بین ژاپن و روسیه\n",
            "بنا به گزارش منتشره در توکیو ژاپن و روسیه در زمینه یک پیمان صلح در چارچوب گسترده توافق کرد#کن که رسما به مخاصمات جنگ دوم جهانی میان دو کشور پایان داد#ده\n",
            "\n",
            "در یک از این گزارش که از سو خبرگزاری کیودوانتشار یافته گفت#گو که دو کشور برای رفع اختلافات دیرین خود بر سر چهار جزیره از جزایر زنجیره کوریل بر اساس سه پیمان گذشته خود عمل کرد#کن\n",
            "بموجب یک از این پیمان که در سال ۱۹۵۶ امضاء شده دو تا از این جزیره پس از امضاء یک پیمان صلح به ژاپن پس داد#ده\n",
            "اما بموجب پیمان که در سال ۱۹۹۳ به امضاء رسیده مسئله حاکمیت این چهار جزیره ایستاد#ایست پیش از امضاء پیمان صلح فیصله یافت#یاب\n",
            "هیچ یک از دو طرف نحوه استفاده از پیمان پیشین را اعلام کرد#کن\n",
            "\n",
            "تشکیلات فلسطین نخستین بودجه رسم خود را اعلام کرد#کن\n",
            "تشکیلات فلسطین پس از دو سال نخستین بودجه رسم خود را اعلام کرد#کن و قول داد برای از میان برداشتن فساد و پاسخگوئی بیشتر به مردم تلاش کند\n"
          ]
        }
      ],
      "source": [
        "!head voa_persian_normalized.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9-rAN6e2tNx"
      },
      "source": [
        "### Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWdUYOwnsXKj"
      },
      "source": [
        "Now we'll use `KenLM` to train an N-gram language model. an N-gram model calculates probability of N words being together.\n",
        "\n",
        "You can read more about N-gram [here](https://towardsdatascience.com/understanding-word-n-grams-and-n-gram-probability-in-natural-language-processing-9d9eef0fa058).\n",
        "\n",
        "First, let's install download and build `KenLM`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtuYeg5Gq2AW",
        "outputId": "53345fc0-b7a7-401d-f4dc-dbb918aec758"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-27 21:38:39--  https://kheafield.com/code/kenlm.tar.gz\n",
            "Resolving kheafield.com (kheafield.com)... 35.196.63.85\n",
            "Connecting to kheafield.com (kheafield.com)|35.196.63.85|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 491888 (480K) [application/x-gzip]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>] 480.36K   422KB/s    in 1.1s    \n",
            "\n",
            "2023-04-27 21:38:41 (422 KB/s) - written to stdout [491888/491888]\n",
            "\n",
            "mkdir: cannot create directory ‘kenlm/build’: File exists\n",
            "-- Could NOT find Eigen3 (missing: Eigen3_DIR)\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/kenlm/build\n",
            "[  1%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum-dtoa.cc.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum.cc.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/cached-powers.cc.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/diy-fp.cc.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/double-conversion.cc.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fast-dtoa.cc.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fixed-dtoa.cc.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/strtod.cc.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/chain.cc.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/count_records.cc.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/io.cc.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/line_input.cc.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/multi_progress.cc.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/rewindable_stream.cc.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/bit_packing.cc.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/ersatz_progress.cc.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/exception.cc.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file.cc.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file_piece.cc.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/float_to_string.cc.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/integer_to_string.cc.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/mmap.cc.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/murmur_hash.cc.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/parallel_read.cc.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/pool.cc.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/read_compressed.cc.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/scoped.cc.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/spaces.cc.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/string_piece.cc.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/usage.cc.o\u001b[0m\n",
            "[ 38%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm_util.a\u001b[0m\n",
            "[ 38%] Built target kenlm_util\n",
            "[ 40%] \u001b[32mBuilding CXX object util/CMakeFiles/probing_hash_table_benchmark.dir/probing_hash_table_benchmark_main.cc.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/bhiksha.cc.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/binary_format.cc.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/config.cc.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/lm_exception.cc.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/model.cc.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/quantize.cc.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/read_arpa.cc.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_hashed.cc.o\u001b[0m\n",
            "[ 51%] \u001b[32m\u001b[1mLinking CXX executable ../bin/probing_hash_table_benchmark\u001b[0m\n",
            "[ 51%] Built target probing_hash_table_benchmark\n",
            "[ 52%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/arpa_io.cc.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/phrase.cc.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_trie.cc.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/vocab.cc.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_filter.a\u001b[0m\n",
            "[ 57%] Built target kenlm_filter\n",
            "[ 58%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/sizes.cc.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie.cc.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie_sort.cc.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/value_build.cc.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/virtual_interface.cc.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/vocab.cc.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/model_buffer.cc.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/print.cc.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/renumber.cc.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/size_option.cc.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm.a\u001b[0m\n",
            "[ 71%] Built target kenlm\n",
            "[ 72%] \u001b[32mBuilding CXX object lm/CMakeFiles/query.dir/query_main.cc.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object lm/CMakeFiles/fragment.dir/fragment_main.cc.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../bin/fragment\u001b[0m\n",
            "[ 75%] Built target fragment\n",
            "[ 76%] \u001b[32mBuilding CXX object lm/CMakeFiles/build_binary.dir/build_binary_main.cc.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/query\u001b[0m\n",
            "[ 77%] Built target query\n",
            "[ 78%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm_benchmark.dir/kenlm_benchmark_main.cc.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../bin/build_binary\u001b[0m\n",
            "[ 80%] Built target build_binary\n",
            "[ 81%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/adjust_counts.cc.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/corpus_count.cc.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/initial_probabilities.cc.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/interpolate.cc.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/output.cc.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../bin/kenlm_benchmark\u001b[0m\n",
            "[ 87%] Built target kenlm_benchmark\n",
            "[ 88%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/filter.dir/filter_main.cc.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/pipeline.cc.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_builder.a\u001b[0m\n",
            "[ 91%] Built target kenlm_builder\n",
            "[ 92%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/phrase_table_vocab.dir/phrase_table_vocab_main.cc.o\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/phrase_table_vocab\u001b[0m\n",
            "[ 93%] Built target phrase_table_vocab\n",
            "[ 95%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/lmplz.dir/lmplz_main.cc.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/filter\u001b[0m\n",
            "[ 96%] Built target filter\n",
            "[ 97%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/count_ngrams.dir/count_ngrams_main.cc.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/lmplz\u001b[0m\n",
            "[ 98%] Built target lmplz\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/count_ngrams\u001b[0m\n",
            "[100%] Built target count_ngrams\n"
          ]
        }
      ],
      "source": [
        "!wget -O - https://kheafield.com/code/kenlm.tar.gz | tar xz; mkdir kenlm/build; cd kenlm/build; cmake ..; make -j2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NB9vhHY3Wkt"
      },
      "source": [
        "Now let's make a 5-gram model using "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wvr9XW2btIp-",
        "outputId": "2681566f-54aa-4dc4-f3b4-295277bbc54f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1/5 Counting and sorting n-grams ===\n",
            "Reading /content/voa_persian_normalized.txt\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Unigram tokens 7151282 types 105479\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:1265748 2:1062614080 3:1992401408 4:3187842048 5:4648936960\n",
            "Statistics:\n",
            "1 105479 D1=0.692798 D2=1.02059 D3+=1.36868\n",
            "2 1273831 D1=0.753634 D2=1.09875 D3+=1.3404\n",
            "3 3442840 D1=0.837136 D2=1.17748 D3+=1.39394\n",
            "4 5019073 D1=0.905517 D2=1.28916 D3+=1.43789\n",
            "5 5610872 D1=0.891831 D2=1.51472 D3+=1.61131\n",
            "Memory estimate for binary LM:\n",
            "type     MB\n",
            "probing 321 assuming -p 1.5\n",
            "probing 377 assuming -r models -p 1.5\n",
            "trie    153 without quantization\n",
            "trie     83 assuming -q 8 -b 8 quantization \n",
            "trie    135 assuming -a 22 array pointer compression\n",
            "trie     66 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "Chain sizes: 1:1265748 2:20381296 3:68856800 4:120457752 5:157104416\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:1265748 2:20381296 3:68856800 4:120457752 5:157104416\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Name:lmplz\tVmPeak:10810012 kB\tVmRSS:29488 kB\tRSSMax:2063144 kB\tuser:14.3706\tsys:4.55888\tCPU:18.9295\treal:18.0517\n"
          ]
        }
      ],
      "source": [
        "!kenlm/build/bin/lmplz -o 5 <\"voa_persian_normalized.txt\"> \"voa_persian.arpa\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHIy7Bo8FDAO",
        "outputId": "5c760134-9e9e-44de-d31e-2295f35fa10f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\data\\\n",
            "ngram 1=105479\n",
            "ngram 2=1273831\n",
            "ngram 3=3442840\n",
            "ngram 4=5019073\n",
            "ngram 5=5610872\n",
            "\n",
            "\\1-grams:\n",
            "-6.138535\t<unk>\t0\n",
            "0\t<s>\t-1.5815679\n",
            "-2.1129756\t</s>\t0\n",
            "-3.5871809\tپیمان\t-0.50824106\n",
            "-3.304699\tصلح\t-0.560521\n",
            "-2.891446\tبین\t-0.67797303\n",
            "-3.303326\tژاپن\t-0.5074899\n",
            "-2.0037236\tو\t-0.8103652\n",
            "-3.097553\tروسیه\t-0.56382215\n",
            "-3.694238\tبنا\t-0.5076225\n",
            "-2.0797832\tبه\t-1.0190648\n",
            "-3.047614\tگزارش\t-0.6365477\n"
          ]
        }
      ],
      "source": [
        "!head -n 20 voa_persian.arpa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkcS4dFtujvo"
      },
      "source": [
        "Now lets extract the list of words and sort them using their probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSR4Cw6st1G7",
        "outputId": "a31a3a59-b168-4bd5-8147-eb4889b08e96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['</s>',\n",
              " 'در',\n",
              " 'و',\n",
              " 'به',\n",
              " 'را',\n",
              " 'که',\n",
              " 'از',\n",
              " 'با',\n",
              " '#است',\n",
              " 'بود#باش',\n",
              " 'یک',\n",
              " 'برای',\n",
              " 'این',\n",
              " 'شد#شو',\n",
              " 'گفت#گو',\n",
              " 'خود',\n",
              " 'آن',\n",
              " 'کرد#کن',\n",
              " 'روز',\n",
              " 'نیز']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "words = []\n",
        "words_started = False\n",
        "with open('voa_persian.arpa') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not words_started:\n",
        "            if line == r'\\1-grams:':\n",
        "                words_started = True\n",
        "        else:\n",
        "            if line == r'\\2-grams:':\n",
        "                words = words[:-1]\n",
        "                break\n",
        "            words.append(line.split())\n",
        "words_sorted = sorted(words, key=lambda x: x[0])\n",
        "words_total = [w[1] for w in words_sorted]\n",
        "words_total.remove('</s>')\n",
        "words_total.insert(0, '</s>')\n",
        "words_total[:20]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxznD7kt3f_T",
        "outputId": "e83a2df5-b248-45a9-96cc-d25a9f3d834d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
            "  Using cached https://github.com/kpu/kenlm/archive/master.zip (553 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install https://github.com/kpu/kenlm/archive/master.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "eIeI7WM13nK4"
      },
      "outputs": [],
      "source": [
        "import kenlm\n",
        "\n",
        "model = kenlm.Model('voa_persian.arpa')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RepmrJXhzmjt"
      },
      "source": [
        "Now we need a measure using our language model to measure how well our sentence fit together. Our model can measure the probability of a sentence using N-gram.\n",
        "\n",
        "This has a downside. the longer the sentence gets, the lower its' probability becomes. We don't want that. So we introduce `perplexity`. a measure which is normalized by the sentence's length. Lower perplexity means the semantics of our sentence fits better together.\n",
        "\n",
        "You can read more about perplexity [here](https://medium.com/nlplanet/two-minutes-nlp-perplexity-explained-with-simple-probabilities-6cdc46884584).\n",
        "$$\n",
        "\\begin{align}\n",
        "PP(S) &= 10 ^ {-\\frac{log(P(S))}{N}} \\\\\n",
        "PP(S) &= \\sqrt[N]{\\frac{1}{P(S)}} \\\\\n",
        "PP(S) &= \\sqrt[N]{\\frac{1}{P(W_1W_2...W_N)}} \\\\\n",
        "PP(S) &= \\sqrt[N]{\\prod_{i=1}^N{\\frac{1}{P(W_i|W_1W_2...W_{i-1})}}}\n",
        "\\end{align}\n",
        "$$\n",
        "**Note**: `KenLM` score function return log10 probability of a sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2Y5NjoHoSNz"
      },
      "source": [
        "### Perplexity (10 Points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "oPH4awk57yOZ"
      },
      "outputs": [],
      "source": [
        "def perplexity(sentence: str):\n",
        "    \"\"\"\n",
        "    returns the perplexity of a sentence using model.score method\n",
        "    Args:\n",
        "      sentence: string of words\n",
        "\n",
        "    Returns:\n",
        "      perplexity: 10^(-lop10p(sentence) / N)\n",
        "    \"\"\"\n",
        "    N = len(sentence.split())\n",
        "    pp = pow(10 , -model.score(sentence)/N)\n",
        "    return pp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "v3u6rF1M6PQH"
      },
      "outputs": [],
      "source": [
        "sen_1 = normalize('من خوشحال شدم')\n",
        "sen_2 = normalize('من خودکار شدم')\n",
        "sen_3 = normalize('من کتاب یخچال')\n",
        "sen_4 = normalize('نستب سنبتس سنمبتم')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sv44dRB_6e4C",
        "outputId": "4100774b-8360-4087-c10c-7fb5bb5d86b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "من خوشحال شد#شو 706.3661171531168\n",
            "من خودکار شد#شو 13076.187677740996\n",
            "من کتاب یخچال 145940.446071158\n",
            "نستب سنبتس سنمبتم 23444976.10881125\n"
          ]
        }
      ],
      "source": [
        "print(sen_1, perplexity(sen_1))\n",
        "print(sen_2, perplexity(sen_2))\n",
        "print(sen_3, perplexity(sen_3))\n",
        "print(sen_4, perplexity(sen_4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_oHC5Vy7ypg"
      },
      "source": [
        "## Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2VRX8HM72PX"
      },
      "source": [
        "### Reward Function (10 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX_ET9KYb4ov"
      },
      "source": [
        "Reward function should give us a reward based on how the last word added to the sentence changed the meaning and how well it fits with the others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "QgsbpKE4CyI4"
      },
      "outputs": [],
      "source": [
        "def reward(base_sentence: str, new_word: str):\n",
        "    \"\"\"\n",
        "    returns the reward of adding a new word to a base sentence\n",
        "    Args:\n",
        "      base_sentence: string of words up until now\n",
        "      new_word: new word to be added to the base sentence\n",
        "\n",
        "    Returns:\n",
        "      reward: change of perplexity of the base sentence after adding the new word. positive reward means the new word is more likely to be added to the base sentence.\n",
        "    \"\"\"\n",
        "    reward=0\n",
        "    if (len(base_sentence.split())) ==0:\n",
        "      reward=+1\n",
        "      return reward\n",
        "    new_sentence=base_sentence+' '+new_word\n",
        "    pp_old=perplexity(base_sentence)\n",
        "    pp_new=perplexity(new_sentence)\n",
        "    \n",
        "    if pp_old<pp_new:\n",
        "      reward = -1\n",
        "    else:\n",
        "      reward = 1\n",
        "    return reward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jk1iYSm73l9",
        "outputId": "0a31c641-6ffb-4849-ebd2-20d91397dcaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "-1\n"
          ]
        }
      ],
      "source": [
        "print(reward('', 'من'))\n",
        "print(reward('من', 'خوشحال'))\n",
        "print(reward('من خوشحال', 'شد#شو'))\n",
        "print(reward('جنگ جهانی', 'اول'))\n",
        "print(reward('جنگ جهانی', 'دوم'))\n",
        "print(reward('جنگ جهانی', 'صورتی'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1ga2l1WczI5"
      },
      "source": [
        "Since we have to implement text generator using a tabular implementation, we have to assume that all that matters in a text is in a window of N words. It matches our language model of N-gram.\n",
        "\n",
        "We model it using MDP. the first state is `<s>` state. it has no text and 0 perplexity. The next state is $W_1$ state. We usually have a negative perplexity because no text has more meaning than a one word sentence. Next is $W_1W_2$ state until we reach $W_1W_2...W_N$ state, from then with our window assumption we go to $W_2W_3...W_{N+1}$ state and $W_3W_4...W_{N+2}$ and so on.\n",
        "\n",
        "First thing we notice is that our search space is **really** big. Each word choice has thousands of possibilites. We cannot model our search space using our normal Q Table.\n",
        "Since our states are sequential and we need to find the best word using our current state, we can use `dict` in `dict` architecture.\n",
        "\n",
        "First we reduce the search space to the 10K most used words.\n",
        "For faster computation, we use each word index for states."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_1gWfK2oSN0"
      },
      "source": [
        "### Utility Functions (10 Points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXXEro4V2T9f",
        "outputId": "fe8d827f-357e-4100-8bbd-63e4841721be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "یک\n",
            "10\n",
            "مقام کارت رئیس .\n",
            "[389, 2887, -1]\n",
            "مقام یک برقرار مهارت\n",
            "[389, 10, 787, -1]\n"
          ]
        }
      ],
      "source": [
        "words = words_total[:10000]\n",
        "# 0 index is for </s> which means end of the sentence.\n",
        "indexes = dict()\n",
        "for i, w in enumerate(words):\n",
        "    indexes[w] = i\n",
        "\n",
        "\n",
        "def index_to_word(index: int):\n",
        "    \"\"\"\n",
        "    returns the word of a given index\n",
        "    Args:\n",
        "        index: index of the word\n",
        "\n",
        "    Returns:\n",
        "        word: word of the given index. '.' if the index is 0 (end of sentence or </s>)\n",
        "    \"\"\"\n",
        "    if index==0: return '.'\n",
        "    return words[index]  \n",
        "\n",
        "\n",
        "\n",
        "def word_to_index(word: str):\n",
        "    \"\"\"\n",
        "    returns the index of a given word\n",
        "    Args:\n",
        "        word: word of the given index. word should be normalized.\n",
        "\n",
        "    Returns:\n",
        "        index: index of the word. -1 if the word is not in the vocabulary\n",
        "    \"\"\"\n",
        "    if word not in indexes: return -1\n",
        "    return indexes[word]\n",
        "\n",
        "\n",
        "def state_to_sentence(state: list[int]):\n",
        "    \"\"\"\n",
        "    returns the sentence of a given state\n",
        "    Args:\n",
        "        state: list of indexes of words\n",
        "\n",
        "    Returns:\n",
        "        sentence: string of words. '.' when the state is 0 (end of sentence or </s>)\n",
        "    \"\"\"\n",
        "    sentence=''\n",
        "    a=0\n",
        "    for index in state:\n",
        "      if a==0:\n",
        "        sentence=index_to_word(index)\n",
        "      else:  \n",
        "         sentence = sentence + ' ' +index_to_word(index)\n",
        "      a+=1\n",
        "    return sentence\n",
        "\n",
        "def sentence_to_state(sentence: str):\n",
        "    \"\"\"\n",
        "    returns the state of a given sentence\n",
        "    Args:\n",
        "        sentence: string of words. sentence should be normalized.\n",
        "\n",
        "    Returns:\n",
        "        state: list of indexes of words. no need to add the index of </s> (end of sentence) to the state\n",
        "    \"\"\"\n",
        "    sen_words=sentence.split()\n",
        "    indes=[]  \n",
        "    for word in sen_words:\n",
        "      indes.append(word_to_index(word))\n",
        "    return indes  \n",
        "\n",
        "\n",
        "\n",
        "print(index_to_word(10))\n",
        "print(word_to_index('یک'))\n",
        "print(state_to_sentence([390, 2884, 24, 0]))\n",
        "print(sentence_to_state('من خوشحال هستم'))\n",
        "print(state_to_sentence([390, 10, 791, 3816]))\n",
        "print(sentence_to_state('من یک کتاب خریدم'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5Jzz59DziLv",
        "outputId": "6057d858-05b5-47de-e554-3ed619932861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q[من] 10\n",
            "Q[من, خوشحال] 20\n",
            "Q[من, خوشحال, هستم] 25\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{389: (10,\n",
              "  {2887: (20, {-1: (25, {0: (0, {})})}),\n",
              "   10: (5, {787: (15, {-1: (10, {})}), 479: (15, {-1: (8, {})})})}),\n",
              " 2172: (10,\n",
              "  {2887: (20, {7601: (7, {0: (0, {})})}),\n",
              "   27: (5, {787: (15, {-1: (11, {})})})})}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# example Q Table\n",
        "q_table = {\n",
        "    word_to_index('من'): (10, {\n",
        "        word_to_index('خوشحال'): (20, {\n",
        "            word_to_index('هستم'): (25, {\n",
        "                0: (0, {}),\n",
        "            }),\n",
        "        }),\n",
        "        word_to_index('یک'): (5, {\n",
        "            word_to_index('کتاب'): (15, {\n",
        "                word_to_index('خریدم'): (10, {}),\n",
        "            }),\n",
        "            word_to_index('گل'): (15, {\n",
        "                word_to_index('دیدم'): (8, {}),\n",
        "            }),\n",
        "        })\n",
        "    }),\n",
        "    word_to_index('تو'): (10, {\n",
        "        word_to_index('خوشحال'): (20, {\n",
        "            word_to_index('هستی'): (7, {\n",
        "                0: (0, {}),\n",
        "            }),\n",
        "        }),\n",
        "        word_to_index('دو'): (5, {\n",
        "            word_to_index('کتاب'): (15, {\n",
        "                word_to_index('خریدی'): (11, {}),\n",
        "            }),\n",
        "        })\n",
        "    }),\n",
        "}\n",
        "print('Q[من]', q_table[word_to_index('من')][0])\n",
        "print('Q[من, خوشحال]', q_table[word_to_index('من')]\n",
        "      [1][word_to_index('خوشحال')][0])\n",
        "print('Q[من, خوشحال, هستم]', q_table[word_to_index('من')][1]\n",
        "      [word_to_index('خوشحال')][1][word_to_index('هستم')][0])\n",
        "q_table\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXd7KKsdoSN1"
      },
      "source": [
        "### Hyperparameters\n",
        "You can change these parameters to get better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "t3RDh4mzyG2k"
      },
      "outputs": [],
      "source": [
        "q_table = {}\n",
        "alpha = 0.8\n",
        "gamma = 0.95\n",
        "state_N = 6\n",
        "N = 75"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTjY6iuyoSN1"
      },
      "source": [
        "### Q-Learning Utility Functions (50 Points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dh9HClfJDQVT"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import bisect\n",
        "\n",
        "weights = [1 for i in range(10000)]\n",
        "\n",
        "\n",
        "def random_index():\n",
        "    \"\"\"\n",
        "    returns a random index based on the weights\n",
        "\n",
        "    Returns:\n",
        "        index: index of the word\n",
        "    \"\"\"\n",
        "    return random.choice(weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "dvtoh1fME_e7"
      },
      "outputs": [],
      "source": [
        "def q_table_max_find(q_table: dict[int, tuple[int, dict]], state: list[int]):\n",
        "    \"\"\"\n",
        "    returns the index of the word with the maximum Q value in the given state. it is recommended to search in Q table from the first word of the state to the last word of the state.\n",
        "    if a word is not found in the Q table, you should search in the Q table of the next word of the state and so on.\n",
        "    so if we don't have Q[W_1W_2...W_N], we search for Q[W_2W_3...W_N] and so on until Q[W_N]. if we don't have Q[W_N], we should return a random index.\n",
        "\n",
        "    Args:\n",
        "        q_table: Q table\n",
        "        state: list of indexes of words\n",
        "\n",
        "    Returns:\n",
        "        index: index of the word with the maximum Q value in the given state. random index if the state is not in the Q table.\n",
        "    \"\"\"\n",
        "    my_qtable=q_table.copy()\n",
        "    Rando=False\n",
        "    q_val=0\n",
        "    statesin=0\n",
        "    for sta in state:\n",
        "      if sta in my_qtable:\n",
        "        statesin+=1\n",
        "        q_val=my_qtable[sta][0]\n",
        "        \n",
        "        if len(my_qtable)>1: my_qtable=my_qtable[sta][1]\n",
        "    if statesin==0: return  random.randint(0, 10000) \n",
        "    max=-1000000\n",
        "    index=0\n",
        "    kk=0\n",
        "    for key in my_qtable:\n",
        "      if kk>0:\n",
        "        if my_qtable[key][0]>max:\n",
        "          max=my_qtable[key][0]\n",
        "          index=key\n",
        "      kk+=1   \n",
        "    return index\n",
        "\n",
        "\n",
        "def q_table_update(q_table1: dict[int, tuple[int, dict]], state: list[int]):\n",
        "    \"\"\"\n",
        "    updates the Q table based on the given state. update the Q[W_1W_2...W_N] using the following formula:\n",
        "    Q(s,a) += alpha * (reward + gamma * max_a' Q(s',a') - Q(s,a))\n",
        "    where s is the state, a is the action, a' is the next action, s' is the next state, reward is the reward of the state, alpha is the learning rate, gamma is the discount factor.\n",
        "    then update the Q[W_1W_2...W_{N-1}] and so on until Q[W_1].\n",
        "    \n",
        "    Args:\n",
        "        q_table: Q table\n",
        "        state: list of indexes of words\n",
        "    \"\"\"\n",
        "    qval=1\n",
        "    q_table1=recursive_qtable(q_table1,qval,state[0],state,0)  \n",
        "\n",
        "    if len(state)>1:q_table_update(q_table1,state[0:len(state)-1])\n",
        "    return q_table1      \n",
        "\n",
        "\n",
        "def recursive_qtable(q_table1,qval,sta,state,index):\n",
        "  if index==len(state):\n",
        "    for key in range(10000-1):\n",
        "      if key in q_table1:\n",
        "        nextq=q_table1[key][1]\n",
        "        max1=-1000000\n",
        "        index2=0\n",
        "        kkk=0\n",
        "        for key1 in nextq:\n",
        "          if kkk>=0:\n",
        "            if nextq[key1][0]>max1:\n",
        "              max1=nextq[key1][0]\n",
        "              index2=nextq[key1][0]\n",
        "          kkk+=1   \n",
        "        if kkk==0:\n",
        "          max1=0   \n",
        "        qval=reward(state_to_sentence(state[0:len(state)]),index_to_word(key))  \n",
        "        q_table1[key][0]+=alpha*(qval+gamma*max1-q_table1[key][0])    \n",
        "      else:\n",
        "        q_table1[key]=[reward(state_to_sentence(state[0:len(state)]),index_to_word(key)),{}]\n",
        "    return q_table1  \n",
        "      \n",
        "  else:\n",
        "      sta=state[index]\n",
        "      if not ((q_table1) is None):\n",
        "        if sta in q_table1:\n",
        "          qval=reward(state_to_sentence(state[0:state.index(sta)]),index_to_word(sta))\n",
        "          q_table1[sta][1]=recursive_qtable(q_table1[sta][1],qval,sta,state,index+1)\n",
        "        else:\n",
        "            \n",
        "            q_table1[sta]=[]\n",
        "            \n",
        "            q_table1[sta].append(reward(state_to_sentence(state[0:state.index(sta)]),index_to_word(sta)))\n",
        "            child_dict={}\n",
        "            q_table1[sta].append(child_dict)\n",
        "            qval=q_table1[sta][0]\n",
        "            q_table1[sta][1]=recursive_qtable(q_table1[sta][1],qval,sta,state,index+1)            \n",
        "      else:\n",
        "        q_table1={}\n",
        "        q_table1[sta]=[]\n",
        "        \n",
        "        q_table1[sta].append(reward(state_to_sentence(state[0:state.index(sta)]),index_to_word(sta)))\n",
        "        child_dict={}\n",
        "        q_table1[sta].append(child_dict)\n",
        "        qval=q_table1[sta][0]\n",
        "        q_table1[sta][1]=recursive_qtable(q_table1[sta][1],qval,sta,state,index+1)\n",
        "      return   q_table1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ok\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4Aw-T9qsgLC",
        "outputId": "640cc655-a40b-4bcf-e0f7-89e0556b94e3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckqTtH1YoSN1"
      },
      "source": [
        "### Training Loop (10 Points)\n",
        "Since search space is really big, we can let our model train for an hour or two and get a good result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "IcLjORGcFDEM",
        "outputId": "8f99314f-e32f-436f-c64a-4b38c3bd3bf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 5/2000 [03:03<20:18:28, 36.65s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-c72a89ee5ac5>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mq_table\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq_table_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstate_N\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-1c247551332f>\u001b[0m in \u001b[0;36mq_table_update\u001b[0;34m(q_table1, state)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mq_table1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecursive_qtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mqval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mq_table_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mq_table1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-1c247551332f>\u001b[0m in \u001b[0;36mq_table_update\u001b[0;34m(q_table1, state)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mq_table1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecursive_qtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mqval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mq_table_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mq_table1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-1c247551332f>\u001b[0m in \u001b[0;36mq_table_update\u001b[0;34m(q_table1, state)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mq_table1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecursive_qtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mqval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mq_table_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mq_table1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-1c247551332f>\u001b[0m in \u001b[0;36mq_table_update\u001b[0;34m(q_table1, state)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mq_table1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecursive_qtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mqval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mq_table_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mq_table1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-1c247551332f>\u001b[0m in \u001b[0;36mq_table_update\u001b[0;34m(q_table1, state)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mq_table1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecursive_qtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mqval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mq_table_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mq_table1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-1c247551332f>\u001b[0m in \u001b[0;36mq_table_update\u001b[0;34m(q_table1, state)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mq_table1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecursive_qtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mqval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mq_table_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mq_table1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-1c247551332f>\u001b[0m in \u001b[0;36mq_table_update\u001b[0;34m(q_table1, state)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \"\"\"\n\u001b[1;32m     48\u001b[0m     \u001b[0mqval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mq_table1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecursive_qtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mqval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mq_table_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-1c247551332f>\u001b[0m in \u001b[0;36mrecursive_qtable\u001b[0;34m(q_table1, qval, sta, state, index)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msta\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mq_table1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m           \u001b[0mqval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_to_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_to_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m           \u001b[0mq_table1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecursive_qtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mqval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-1c247551332f>\u001b[0m in \u001b[0;36mrecursive_qtable\u001b[0;34m(q_table1, qval, sta, state, index)\u001b[0m\n\u001b[1;32m     66\u001b[0m               \u001b[0mmax1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnextq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m               \u001b[0mindex2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnextq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m           \u001b[0mkkk\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkkk\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m           \u001b[0mmax1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "q_table = {}\n",
        "alpha = 0.8\n",
        "gamma = 0.95\n",
        "state_N = 6\n",
        "N = 75\n",
        "episodes = 2000\n",
        "epsilon = 1\n",
        "episode_N = 75\n",
        "for ep in tqdm(range(episodes)):\n",
        "  state = []\n",
        "  for i in range(episode_N):\n",
        "    if random.random() < epsilon:\n",
        "      #TODO: random action\n",
        "      state.append(random.choice(range(10000-1)))    \n",
        "    else:\n",
        "      a=q_table_max_find(q_table, state)\n",
        "      state.append(a)\n",
        "    # to avoid infinite loop\n",
        "    if len(state) > 1 and state[-1] == state[-2]:\n",
        "      break\n",
        "    q_table=q_table_update(q_table, state)\n",
        "    if len(state) > state_N:\n",
        "      state = state[1:]\n",
        "    if state[-1] == 0:\n",
        "      break\n",
        "  epsilon *= 0.99975\n",
        "\n",
        "print(q_table)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(q_table))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbBkv1Y7XFok",
        "outputId": "7c73bb31-0489-4e4d-f408-9a52ced6ddd7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_KUGhWkoSN2"
      },
      "source": [
        "### Testing (10 Points)\n",
        "This will be the final output of our model. score will be based on how well the output fits with the corpus. Generated sentences should have some meaning in the neighborhood of each word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSpB15EdFKjm",
        "outputId": "6139d2c5-9f1f-455a-afa0-3914e5030574"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ما درحالی دراطراف آنجلو بخار اسفندیار در در \n",
            "یک بازگشته‌اند In علا محرم قوا ٧٨ والیبال آتلانتیک در در \n",
            "ایران کارمند بازرسی توریست‌ها ندا طرفه در در \n"
          ]
        }
      ],
      "source": [
        "def get_result(state, steps=75):\n",
        "    for i in range(steps):\n",
        "        state.append(q_table_max_find(q_table, state))\n",
        "        if state[-1] == 0:\n",
        "            break\n",
        "        if len(state) > state_N:\n",
        "            state = state[1:]\n",
        "        yield state[-1]\n",
        "\n",
        "state = sentence_to_state('ما')\n",
        "print('ما', end=' ')\n",
        "for s in get_result(state):\n",
        "    print(words[s], end=' ')\n",
        "print()\n",
        "state = sentence_to_state('یک')\n",
        "print('یک', end=' ')\n",
        "for s in get_result(state):\n",
        "    print(words[s], end=' ')\n",
        "print()\n",
        "state = sentence_to_state('ایران')\n",
        "print('ایران', end=' ')\n",
        "for s in get_result(state):\n",
        "    print(words[s], end=' ')\n",
        "print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "026f2bef8fb7be59296f2f39e2043bb013bc567dc5026fb77125b1034979614d"
      }
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}